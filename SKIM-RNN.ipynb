{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def load_tsv(filename):\n",
    "    phID, stcID, ph, sentiment = [],[],[],[]\n",
    "    with open(filename) as f: \n",
    "        l = f.readline()\n",
    "        if (len(l.split(\"\\t\")) == 4):\n",
    "            '''TRAIN dataset'''\n",
    "            for line in f:\n",
    "                l = line.split(\"\\t\")\n",
    "                if (int(l[3][0]) < 2):\n",
    "                    phID.append(l[0]), stcID.append(l[1]), ph.append(l[2]), sentiment.append(0)\n",
    "                else:\n",
    "                    phID.append(l[0]), stcID.append(l[1]), ph.append(l[2]), sentiment.append(1)\n",
    "        else:\n",
    "            for line in f:\n",
    "                l = line.split(\"\\t\")\n",
    "                phID.append(l[0]), stcID.append(l[1]), ph.append(l[2])\n",
    "                \n",
    "    return np.array(phID).astype(int), np.array(stcID).astype(int), np.array(ph), np.array(sentiment).astype(int)\n",
    "\n",
    "        \n",
    "    \n",
    "phraseID, sentenceID, sentences, sentiment = load_tsv(\"SKIM-RNN/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trim, Store, Count, Index words from dataset'''\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Preprocesser():\n",
    "    \n",
    "    '''main functions'''\n",
    "    def __init__(self,corpus):\n",
    "        '''corpus : np_array(string)'''\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "        self.corpus = corpus\n",
    "        self.size = len(corpus)\n",
    "    \n",
    "    # Lowercase, trim, and remove non-letter characters\n",
    "    # (no stop words in skim rnn ?)\n",
    "    def normalize(self):\n",
    "        new_corpus = np.array([])\n",
    "        steps,i = np.arange(0,self.size,self.size/10),0\n",
    "        for s in self.corpus:\n",
    "            uni_s = s.tostring().decode('unicode-escape')\n",
    "            uni_s = self.unicodeToAscii(uni_s.lower().strip())\n",
    "            uni_s = re.sub(r\"([.!?])\", r\" \\1\", uni_s)\n",
    "            uni_s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", uni_s)\n",
    "            new_corpus = np.append(new_corpus, uni_s)\n",
    "            if (i in steps):\n",
    "                print (\"...\")\n",
    "            i+=1\n",
    "        self.corpus = new_corpus\n",
    "\n",
    "\n",
    "    def addSentences(self):\n",
    "        for sentence in self.corpus:\n",
    "            self.addSentence(sentence)\n",
    "        \n",
    "    '''called within the module:'''\n",
    "    def unicodeToAscii(self,s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "    def save(self,filename):\n",
    "        pkl.dump(self.corpus,open(filename+\"_corpus.pkl\",'wb'))\n",
    "        pkl.dump(self.word2count,open(filename+\"_w2c.pkl\",'wb'))\n",
    "        pkl.dump(self.word2index,open(filename+\"_w2i.pkl\",'wb'))\n",
    "        pkl.dump(self.index2word,open(filename+\"_i2w.pkl\", 'wb'))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "preprocesser = Preprocesser(sentences)\n",
    "preprocesser.normalize()\n",
    "preprocesser.addSentences()\n",
    "\n",
    "preprocesser.save(\"preprocessing_IMDB\")\n",
    "\n",
    "\n",
    "#corpus = pkl.load(\"preprocessing_IMDB_corpus.pkl\")\n",
    "#w2c = pkl.load(\"preprocessing_IMDB_w2c.pkl\")\n",
    "#w2i = pkl.load(\"preprocessing_IMDB_w2i.pkl\")\n",
    "#i2w = pkl.load(\"preprocessing_IMDB_i2w.pkl\")\n",
    "\n",
    "#preprocesser = Preprocesser(corpus)\n",
    "#preprocesser.word2count = w2c\n",
    "#preprocesser.index2word = i2w\n",
    "#preprocesser.word2index = w2i\n",
    "#preprocesser.n_words = len(word2count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RNN module : embedding layer -> gru layer -> linear layer (for output classification) -> softmax'''\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "        self.h20 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1 , 1, -1)\n",
    "        output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "            output = self.h20(output)\n",
    "            output = self.softmax(output)\n",
    "            #lstm - > (input, (hidden,c <- ???) , cf pytorch doc, 'initial cell state')\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    return result\n",
    "\n",
    "\n",
    "def makeInputTarget(lang, sentence, target, n_classes=2):\n",
    "    input_variable = variableFromSentence(lang, sentence)\n",
    "    if target >= n_classes:\n",
    "        print 'target not in range (0, #classes - 1)'\n",
    "        return -1\n",
    "    target_variable = Variable(torch.LongTensor([target]))\n",
    "    return (input_variable, target_variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input,target,rnn, optimizer, criterion):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    input_length = input.size()[0]\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        output, hidden = rnn(input[ei], hidden)\n",
    "    \n",
    "    loss = criterion(output.view(1,-1), target)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return output ,loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "rnn =  RNN(preprocesser.n_words, 128, 2)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=1e-1)\n",
    "losses = []\n",
    "for i in range(500):\n",
    "    \n",
    "    s,t =preprocesser.corpus[i], sentiment[i]\n",
    "    input,target =  makeInputTarget(preprocesser, s, t)\n",
    "    \n",
    "    output, l = train(input,target,rnn,optimizer,criterion)\n",
    "    losses.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc2182a3f10>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADvJJREFUeJzt21+MXGd5x/Hvr94mFGjzP2DsmA2KpWLUNrSjBASVUiDBQQVHbS6cVsIXqXxDJCitWkeoTRO4IFVbI9QU1SKoFqpIKC3CDapc45Cbqg0ZkwAxwXhJg7J1hI1sgiJUUsPTizmO9l2Ns5ud8Q67+/1Ioznve56Zed7JxL8958ykqpAk6Yyfm3QDkqSfLQaDJKlhMEiSGgaDJKlhMEiSGgaDJKlhMEiSGgaDJKlhMEiSGlOTbmApLr300pqenp50G5K0ohw6dOj7VXXZQnUrMhimp6fp9/uTbkOSVpQk311MnaeSJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1BhLMCTZmuRIkpkku4bsPz/J/d3+h5NMz9u/KclzSf54HP1IkpZu5GBIsg64B7gR2ALckmTLvLJbgVNVdRWwG7h73v7dwL+N2oskaXTjOGK4Bpipqier6nngPmDbvJptwN5u+3PA25MEIMlNwJPA4TH0Ikka0TiCYQPw9JzxbDc3tKaqTgPPApckeQXwp8CdY+hDkjQG4wiGDJmrRdbcCeyuqucWfJFkZ5J+kv6JEyeW0KYkaTGmxvAcs8AVc8YbgWNnqZlNMgVcAJwErgVuTvKXwIXAT5P8b1X97fwXqao9wB6AXq83P3gkSWMyjmB4BNic5Ergf4DtwO/Nq9kH7AD+E7gZeLCqCvjNMwVJ/gJ4blgoSJKWz8jBUFWnk9wG7AfWAZ+qqsNJ7gL6VbUPuBf4dJIZBkcK20d9XUnSuZHBH+4rS6/Xq36/P+k2JGlFSXKoqnoL1fnLZ0lSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSYyzBkGRrkiNJZpLsGrL//CT3d/sfTjLdzV+f5FCSb3T3bxtHP5KkpRs5GJKsA+4BbgS2ALck2TKv7FbgVFVdBewG7u7mvw+8u6p+BdgBfHrUfiRJoxnHEcM1wExVPVlVzwP3Advm1WwD9nbbnwPeniRV9WhVHevmDwMvS3L+GHqSJC3ROIJhA/D0nPFsNze0pqpOA88Cl8yr+V3g0ar68Rh6kiQt0dQYniND5uql1CR5A4PTSzec9UWSncBOgE2bNr30LiVJizKOI4ZZ4Io5443AsbPVJJkCLgBOduONwOeB91bVd872IlW1p6p6VdW77LLLxtC2JGmYcQTDI8DmJFcmOQ/YDuybV7OPwcVlgJuBB6uqklwIfBG4var+Ywy9SJJGNHIwdNcMbgP2A08An62qw0nuSvKeruxe4JIkM8AHgTNfab0NuAr4sySPdbfLR+1JkrR0qZp/OeBnX6/Xq36/P+k2JGlFSXKoqnoL1fnLZ0lSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDUMBklSw2CQJDXGEgxJtiY5kmQmya4h+89Pcn+3/+Ek03P23d7NH0nyznH0I0laupGDIck64B7gRmALcEuSLfPKbgVOVdVVwG7g7u6xW4DtwBuArcDfdc8nSZqQcRwxXAPMVNWTVfU8cB+wbV7NNmBvt/054O1J0s3fV1U/rqr/Bma655MkTcjUGJ5jA/D0nPEscO3ZaqrqdJJngUu6+f+a99gNY+hpqDv/9TDfPPbDc/X0knRObXnNL3HHu99wzl9nHEcMGTJXi6xZzGMHT5DsTNJP0j9x4sRLbFGStFjjOGKYBa6YM94IHDtLzWySKeAC4OQiHwtAVe0B9gD0er2h4bGQ5UhaSVrpxnHE8AiwOcmVSc5jcDF537yafcCObvtm4MGqqm5+e/etpSuBzcBXxtCTJGmJRj5i6K4Z3AbsB9YBn6qqw0nuAvpVtQ+4F/h0khkGRwrbu8ceTvJZ4JvAaeB9VfWTUXuSJC1dBn+4ryy9Xq/6/f6k25CkFSXJoarqLVTnL58lSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSQ2DQZLUMBgkSY2RgiHJxUkOJDna3V90lrodXc3RJDu6uZcn+WKSbyU5nOSjo/QiSRqPUY8YdgEHq2ozcLAbN5JcDNwBXAtcA9wxJ0D+qqp+GXgj8JYkN47YjyRpRKMGwzZgb7e9F7hpSM07gQNVdbKqTgEHgK1V9aOq+jJAVT0PfBXYOGI/kqQRjRoMr6qqZwC6+8uH1GwAnp4znu3mXpDkQuDdDI46JEkTNLVQQZIvAa8esutDi3yNDJmrOc8/BXwG+HhVPfkifewEdgJs2rRpkS8tSXqpFgyGqnrH2fYl+V6S9VX1TJL1wPEhZbPAdXPGG4GH5oz3AEer6mML9LGnq6XX69WL1UqSlm7UU0n7gB3d9g7gC0Nq9gM3JLmou+h8QzdHko8AFwAfGLEPSdKYjBoMHwWuT3IUuL4bk6SX5JMAVXUS+DDwSHe7q6pOJtnI4HTUFuCrSR5L8gcj9iNJGlGqVt5ZmV6vV/1+f9JtSNKKkuRQVfUWqvOXz5KkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWoYDJKkhsEgSWqMFAxJLk5yIMnR7v6is9Tt6GqOJtkxZP++JI+P0oskaTxGPWLYBRysqs3AwW7cSHIxcAdwLXANcMfcAEnyO8BzI/YhSRqTUYNhG7C3294L3DSk5p3Agao6WVWngAPAVoAkrwQ+CHxkxD4kSWMyajC8qqqeAejuLx9SswF4es54tpsD+DDw18CPRuxDkjQmUwsVJPkS8Oohuz60yNfIkLlKcjVwVVX9YZLpRfSxE9gJsGnTpkW+tCTppVowGKrqHWfbl+R7SdZX1TNJ1gPHh5TNAtfNGW8EHgLeDPxGkqe6Pi5P8lBVXccQVbUH2APQ6/Vqob4lSUsz6qmkfcCZbxntAL4wpGY/cEOSi7qLzjcA+6vqE1X1mqqaBt4KfPtsoSBJWj6jBsNHgeuTHAWu78Yk6SX5JEBVnWRwLeGR7nZXNydJ+hmUqpV3VqbX61W/3590G5K0oiQ5VFW9her85bMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqWEwSJIaBoMkqZGqmnQPL1mSE8B3l/jwS4Hvj7GdlcA1rw2ueW0YZc2vrarLFipakcEwiiT9qupNuo/l5JrXBte8NizHmj2VJElqGAySpMZaDIY9k25gAlzz2uCa14ZzvuY1d41BkvTi1uIRgyTpRayZYEiyNcmRJDNJdk26n3FK8qkkx5M8Pmfu4iQHkhzt7i/q5pPk49378PUkvz65zpcmyRVJvpzkiSSHk7y/m1/Na35Zkq8k+Vq35ju7+SuTPNyt+f4k53Xz53fjmW7/9CT7H0WSdUkeTfJAN17Va07yVJJvJHksSb+bW9bP9poIhiTrgHuAG4EtwC1Jtky2q7H6B2DrvLldwMGq2gwc7MYweA82d7edwCeWqcdxOg38UVW9HngT8L7uv+dqXvOPgbdV1a8BVwNbk7wJuBvY3a35FHBrV38rcKqqrgJ2d3Ur1fuBJ+aM18Kaf6uqrp7ztdTl/WxX1aq/AW8G9s8Z3w7cPum+xrzGaeDxOeMjwPpuez1wpNv+e+CWYXUr9QZ8Abh+rawZeDnwVeBaBj90murmX/icA/uBN3fbU11dJt37Eta6kcE/hG8DHgCyBtb8FHDpvLll/WyviSMGYAPw9JzxbDe3mr2qqp4B6O4v7+ZX1XvRnS54I/Awq3zN3SmVx4DjwAHgO8APqup0VzJ3XS+sudv/LHDJ8nY8Fh8D/gT4aTe+hNW/5gL+PcmhJDu7uWX9bE+N+gQrRIbMrdWvY62a9yLJK4F/Bj5QVT9Mhi1tUDpkbsWtuap+Alyd5ELg88Drh5V19yt+zUl+GzheVYeSXHdmekjpqllz5y1VdSzJ5cCBJN96kdpzsua1csQwC1wxZ7wRODahXpbL95KsB+juj3fzq+K9SPLzDELhH6vqX7rpVb3mM6rqB8BDDK6vXJjkzB94c9f1wpq7/RcAJ5e305G9BXhPkqeA+xicTvoYq3vNVNWx7v44gz8ArmGZP9trJRgeATZ332Y4D9gO7JtwT+faPmBHt72DwXn4M/Pv7b7N8Cbg2TOHqCtFBocG9wJPVNXfzNm1mtd8WXekQJJfAN7B4ILsl4Gbu7L5az7zXtwMPFjdSeiVoqpur6qNVTXN4P/ZB6vq91nFa07yiiS/eGYbuAF4nOX+bE/6QssyXtB5F/BtBudlPzTpfsa8ts8AzwD/x+AviFsZnFs9CBzt7i/uasPgG1rfAb4B9Cbd/xLW+1YGh8tfBx7rbu9a5Wv+VeDRbs2PA3/ezb8O+AowA/wTcH43/7JuPNPtf92k1zDi+q8DHljta+7W9rXudvjMv1XL/dn2l8+SpMZaOZUkSVokg0GS1DAYJEkNg0GS1DAYJEkNg0GS1DAYJEkNg0GS1Ph/JSKGGKKI+J4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2184be450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

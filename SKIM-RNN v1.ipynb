{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importer données rotten tomatoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "#load RottenTomatoes sentiment analysis dataset\n",
    "def load_tsv(filename):\n",
    "    phID, stcID, ph, sentiment = [],[],[],[]\n",
    "    with open(filename) as f: \n",
    "        l = f.readline()\n",
    "        if (len(l.split(\"\\t\")) == 4):\n",
    "            '''TRAIN dataset'''\n",
    "            for line in f:\n",
    "                l = line.split(\"\\t\")\n",
    "                if (int(l[3][0]) < 2):\n",
    "                    phID.append(l[0]), stcID.append(l[1]), ph.append(l[2]), sentiment.append(0)\n",
    "                else:\n",
    "                    phID.append(l[0]), stcID.append(l[1]), ph.append(l[2]), sentiment.append(1)\n",
    "        else:\n",
    "            for line in f:\n",
    "                l = line.split(\"\\t\")\n",
    "                phID.append(l[0]), stcID.append(l[1]), ph.append(l[2])\n",
    "                \n",
    "    return np.array(phID).astype(int), np.array(stcID).astype(int), np.array(ph), np.array(sentiment).astype(int)\n",
    "\n",
    "#make validation dataset\n",
    "def split_train_test(data, percentage=80):\n",
    "    size = len(data)\n",
    "    indexes = np.arange(0,size)\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    train_indexes = indexes[0:int(size*percentage/100)]\n",
    "    test_indexes = indexes[int(size*percentage/100):-1]\n",
    "    \n",
    "    return train_indexes, test_indexes\n",
    "    \n",
    "phraseID, sentenceID, sentences, sentiment = load_tsv(\"SKIM-RNN/train.tsv\")\n",
    "train_indexes, test_indexes = split_train_test(phraseID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trim, Store, Count, Index words from dataset'''\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Preprocesser():\n",
    "    \n",
    "    '''main functions'''\n",
    "    def __init__(self,corpus):\n",
    "        '''corpus : np_array(string)'''\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "        self.corpus = corpus\n",
    "        self.size = len(corpus)\n",
    "    \n",
    "    # Lowercase, trim, and remove non-letter characters\n",
    "    # (no stop words in skim rnn ?)\n",
    "    def normalize(self):\n",
    "        new_corpus = np.array([])\n",
    "        steps,i = np.arange(0,self.size,self.size/10),0\n",
    "        for s in self.corpus:\n",
    "            uni_s = s.tostring().decode('unicode-escape')\n",
    "            uni_s = self.unicodeToAscii(uni_s.lower().strip())\n",
    "            uni_s = re.sub(r\"([.!?])\", r\" \\1\", uni_s)\n",
    "            uni_s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", uni_s)\n",
    "            new_corpus = np.append(new_corpus, uni_s)\n",
    "            if (i in steps):\n",
    "                print (\"...\")\n",
    "            i+=1\n",
    "        self.corpus = new_corpus\n",
    "\n",
    "\n",
    "    def addSentences(self):\n",
    "        for sentence in self.corpus:\n",
    "            self.addSentence(sentence)\n",
    "        \n",
    "    '''called within the module:'''\n",
    "    def unicodeToAscii(self,s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "    def save(self,filename):\n",
    "        pkl.dump(self.corpus,open(filename+\"_corpus.pkl\",'wb'))\n",
    "        pkl.dump(self.word2count,open(filename+\"_w2c.pkl\",'wb'))\n",
    "        pkl.dump(self.word2index,open(filename+\"_w2i.pkl\",'wb'))\n",
    "        pkl.dump(self.index2word,open(filename+\"_i2w.pkl\", 'wb'))\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.corpus = pkl.load(open(filename+\"_corpus.pkl\", 'rb'))\n",
    "        self.word2count = pkl.load(open(filename+\"_w2c.pkl\", 'rb'))\n",
    "        self.word2index = pkl.load(open(filename+\"_w2i.pkl\", 'rb'))\n",
    "        self.index2word = pkl.load(open(filename+\"_i2w.pkl\", 'rb'))\n",
    "        self.size = len(self.corpus)\n",
    "        self.n_words = len(self.word2count.keys())\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprocesser = Preprocesser(sentences)\n",
    "#preprocesser.normalize()\n",
    "#preprocesser.addSentences()\n",
    "\n",
    "#preprocesser.save(\"preprocessing_IMDB\")\n",
    "\n",
    "\n",
    "preprocesser = Preprocesser({})\n",
    "preprocesser.load(\"preprocessing_IMDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RNN module : embedding layer -> gru layer -> linear layer (for output classification) -> softmax'''\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        \n",
    "        self.h20 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #embedded = self.embedding(input).view(1 , 1, -1)\n",
    "        #output = embedded\n",
    "        for i in range(self.n_layers):\n",
    "            output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "            output = self.h20(output)\n",
    "            output = self.softmax(output.view(1,-1))\n",
    "            #lstm - > (input, (hidden,c <- ???) , cf pytorch doc, 'initial cell state')\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.rand(1, 1, self.hidden_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "class SkimRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, d, dprime, output_size, k  ):\n",
    "        super(SkimRNN, self).__init__()\n",
    "        self.softmax_p = nn.LogSoftmax()\n",
    "        self.linear_p = nn.Linear(input_size+d, k)\n",
    "        self.mainRNN = RNN(input_size, d, output_size)\n",
    "        self.smallRNN = RNN(input_size, dprime, output_size)\n",
    "        self.d = d\n",
    "        self.dprime = dprime\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        cat = torch.cat( (input.view(1, 1, -1), hidden), 2)\n",
    "        linp = self.linear_p(cat)\n",
    "        p = self.softmax_p(self.linear_p(cat).view(1,-1))\n",
    "        q = p.exp().multinomial()\n",
    "        print q.data[0,0]\n",
    "        if q.data[0,0]==0:\n",
    "            #main RNN\n",
    "            output, hidden = self.mainRNN(input, hidden)\n",
    "        else:\n",
    "            #small RNN\n",
    "            output, h = self.smallRNN(input, hidden)\n",
    "            hidden = torch.cat( (h, hidden[self.dprime+1:-1]), 2)\n",
    "        return output, p, hidden\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.rand(1, 1, self.d))\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions pour la gestion des inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(data, n_words, neg_value=0):\n",
    "    \"\"\"\n",
    "    encode target en one-hot, si neg_value \n",
    "    vaut zéro, ou en -1/1, si neg_value vaut\n",
    "    -1 par exemple\n",
    "    \"\"\"\n",
    "    y_onehot = torch.FloatTensor(n_words)\n",
    "    y_onehot.zero_().add_(neg_value)\n",
    "    return y_onehot.scatter_(0, data, 1)\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = torch.LongTensor(indexes).view(-1, 1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def makeInputTarget(lang, sentence, target, n_classes=2):\n",
    "    input_variable = variableFromSentence(lang, sentence)\n",
    "    if target >= n_classes:\n",
    "        print 'target not in range (0, #classes - 1)'\n",
    "        return -1\n",
    "    target_variable = Variable(torch.LongTensor([target]))\n",
    "    return (input_variable, target_variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel():\n",
    "    return -torch.log(-torch.log(torch.FloatTensor(2).uniform_()))\n",
    "\n",
    "def r(logp, g, temperature):\n",
    "    num = [torch.exp( (pi + gi)/temperature) for (pi,gi) in zip(logp,g)]\n",
    "    denum = torch.sum(torch.exp( (logp[0].data + g)/temperature))\n",
    "    return [n.data/denum for n in num]\n",
    "\n",
    "\n",
    "def train(preprocesser, input,target,rnn, optimizer, criterion, temperature):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    input_length = input.size()[0]\n",
    "    logp = []\n",
    "    rp = []\n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(input_length):\n",
    "        input = binarize(input[t], preprocesser.n_words)\n",
    "        \n",
    "        \n",
    "        output, p, hidden = rnn(Variable(input), hidden)\n",
    "        logp.append(p)\n",
    "    \n",
    "        g = gumbel()    \n",
    "        rp.append(r(logp,g,temperature))\n",
    "    \n",
    "        c = criterion(output.view(1,-1), target)\n",
    "        print c.data\n",
    "        print np.prod(rp[t])\n",
    "        loss += criterion(output.view(1,-1), target).data * np.prod(rp[t])\n",
    "    loss += (gamma/input_length) * np.sum(logp[:,1])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return output ,loss.data[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamètres + apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      " 0.5917\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 0.2754  0.2879\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scatter_ received an invalid combination of arguments - got (int, float, int), but expected one of:\n * (int dim, torch.LongTensor index, float value)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[31;1mfloat\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (int dim, torch.LongTensor index, torch.FloatTensor src)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[31;1mfloat\u001b[0m, \u001b[31;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-e127b9cd6cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpreprocesser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmakeInputTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocesser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocesser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-27752f6eef64>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(preprocesser, input, target, rnn, optimizer, criterion, temperature)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocesser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-3fd3d9f100f0>\u001b[0m in \u001b[0;36mbinarize\u001b[0;34m(data, n_words, neg_value)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0my_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: scatter_ received an invalid combination of arguments - got (int, float, int), but expected one of:\n * (int dim, torch.LongTensor index, float value)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[31;1mfloat\u001b[0m, \u001b[32;1mint\u001b[0m)\n * (int dim, torch.LongTensor index, torch.FloatTensor src)\n      didn't match because some of the arguments have invalid types: (\u001b[32;1mint\u001b[0m, \u001b[31;1mfloat\u001b[0m, \u001b[31;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "rnn =  SkimRNN(preprocesser.n_words, 128, 5, 2, 2)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=1e-2)\n",
    "epoch,mod = 0,10\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "for i in train_indexes[:10]:\n",
    "    \n",
    "    s,t =preprocesser.corpus[i], sentiment[i]\n",
    "    input,target =  makeInputTarget(preprocesser, s, t)\n",
    "    output, l = train(preprocesser,input,target,rnn,optimizer,criterion,1)\n",
    "    \n",
    "    losses.append(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- embedded ? à supprimer ou à faire dans skim rnn plutôt que dans les RNN\n",
    "- debug petit RNN (hidden state vs GRU)\n",
    "- mieux gérer one hot (mot courant)\n",
    "- preprocessing fait façon tuto pytorch, cf GlOVe\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

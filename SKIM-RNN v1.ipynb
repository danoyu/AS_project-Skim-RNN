{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importer données rotten tomatoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle as pkl\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "#load RottenTomatoes sentiment analysis dataset\n",
    "def load_tsv(filename):\n",
    "    phID, stcID, ph, sentiment = [],[],[],[]\n",
    "    with open(filename) as f: \n",
    "        l = f.readline()\n",
    "        if (len(l.split(\"\\t\")) == 4):\n",
    "            '''TRAIN dataset'''\n",
    "            for line in f:\n",
    "                l = line.split(\"\\t\")\n",
    "                if (int(l[3][0]) < 2):\n",
    "                    phID.append(l[0]), stcID.append(l[1]), ph.append(l[2]), sentiment.append(0)\n",
    "                else:\n",
    "                    phID.append(l[0]), stcID.append(l[1]), ph.append(l[2]), sentiment.append(1)\n",
    "        else:\n",
    "            for line in f:\n",
    "                l = line.split(\"\\t\")\n",
    "                phID.append(l[0]), stcID.append(l[1]), ph.append(l[2])\n",
    "                \n",
    "    return np.array(phID).astype(int), np.array(stcID).astype(int), np.array(ph), np.array(sentiment).astype(int)\n",
    "\n",
    "#make validation dataset\n",
    "def split_train_test(data, percentage=80):\n",
    "    size = len(data)\n",
    "    indexes = np.arange(0,size)\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    train_indexes = indexes[0:int(size*percentage/100)]\n",
    "    test_indexes = indexes[int(size*percentage/100):-1]\n",
    "    \n",
    "    return train_indexes, test_indexes\n",
    "    \n",
    "phraseID, sentenceID, sentences, sentiment = load_tsv(\"SKIM-RNN/train.tsv\")\n",
    "train_indexes, test_indexes = split_train_test(phraseID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module de preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trim, Store, Count, Index words from dataset'''\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Preprocesser():\n",
    "    \n",
    "    '''main functions'''\n",
    "    def __init__(self,corpus):\n",
    "        '''corpus : np_array(string)'''\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "        self.corpus = corpus\n",
    "        self.size = len(corpus)\n",
    "    \n",
    "    # Lowercase, trim, and remove non-letter characters\n",
    "    # (no stop words in skim rnn ?)\n",
    "    def normalize(self):\n",
    "        new_corpus = np.array([])\n",
    "        steps,i = np.arange(0,self.size,self.size/10),0\n",
    "        for s in self.corpus:\n",
    "            uni_s = s.tostring().decode('unicode-escape')\n",
    "            uni_s = self.unicodeToAscii(uni_s.lower().strip())\n",
    "            uni_s = re.sub(r\"([.!?])\", r\" \\1\", uni_s)\n",
    "            uni_s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", uni_s)\n",
    "            new_corpus = np.append(new_corpus, uni_s)\n",
    "            if (i in steps):\n",
    "                print (\"...\")\n",
    "            i+=1\n",
    "        self.corpus = new_corpus\n",
    "\n",
    "\n",
    "    def addSentences(self):\n",
    "        for sentence in self.corpus:\n",
    "            self.addSentence(sentence)\n",
    "        \n",
    "    '''called within the module:'''\n",
    "    def unicodeToAscii(self,s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "    def save(self,filename):\n",
    "        pkl.dump(self.corpus,open(filename+\"_corpus.pkl\",'wb'))\n",
    "        pkl.dump(self.word2count,open(filename+\"_w2c.pkl\",'wb'))\n",
    "        pkl.dump(self.word2index,open(filename+\"_w2i.pkl\",'wb'))\n",
    "        pkl.dump(self.index2word,open(filename+\"_i2w.pkl\", 'wb'))\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.corpus = pkl.load(open(filename+\"_corpus.pkl\", 'rb'))\n",
    "        self.word2count = pkl.load(open(filename+\"_w2c.pkl\", 'rb'))\n",
    "        self.word2index = pkl.load(open(filename+\"_w2i.pkl\", 'rb'))\n",
    "        self.index2word = pkl.load(open(filename+\"_i2w.pkl\", 'rb'))\n",
    "        self.size = len(self.corpus)\n",
    "        self.n_words = len(self.word2count.keys())\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions pour la gestion des inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(data, n_words, neg_value=0):\n",
    "    \"\"\"\n",
    "    encode target en one-hot, si neg_value \n",
    "    vaut zéro, ou en -1/1, si neg_value vaut\n",
    "    -1 par exemple\n",
    "    \"\"\"\n",
    "    y_onehot = torch.FloatTensor(n_words)\n",
    "    y_onehot.zero_().add_(neg_value)\n",
    "    return y_onehot.scatter_(0, data, 1)\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variableFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    result = torch.LongTensor(indexes).view(-1, 1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def makeInputTarget(lang, sentence, target, n_classes=2):\n",
    "    input_variable = variableFromSentence(lang, sentence)\n",
    "    if target >= n_classes:\n",
    "        print 'target not in range (0, #classes - 1)'\n",
    "        return -1\n",
    "    target_variable = Variable(torch.LongTensor([target]))\n",
    "    return (input_variable, target_variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprocesser = Preprocesser(sentences)\n",
    "#preprocesser.normalize()\n",
    "#preprocesser.addSentences()\n",
    "\n",
    "#preprocesser.save(\"preprocessing_IMDB\")\n",
    "\n",
    "\n",
    "preprocesser = Preprocesser({})\n",
    "preprocesser.load(\"preprocessing_IMDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RNN module : embedding layer -> gru layer -> linear layer (for output classification) -> softmax'''\n",
    "\n",
    "\n",
    "class Selector(nn.Module):\n",
    "    '''Linear transformation and logsoftmax for the RNN choice'''\n",
    "    def __init__(self, input_size, n_choices=2):\n",
    "        super(Selector, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.n_choices = n_choices\n",
    "        self.linear = nn.Linear(input_size, n_choices)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "    \n",
    "\n",
    "class RNN(nn.Module):\n",
    "    '''Standard RNN with gru'''\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        \n",
    "        self.h20 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.h20(output)\n",
    "        output = self.softmax(output.view(1,-1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.rand(1, 1, self.hidden_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "class SkimRNN(nn.Module):\n",
    "    '''Skim RNN model: a selector and two RNNs modules'''\n",
    "    def __init__(self, input_size, embedding_size, d, dprime, output_size, k  ):\n",
    "        super(SkimRNN, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        self.selector = Selector(embedding_size + d, n_choices=k)\n",
    "        self.mainRNN = RNN(embedding_size, d, output_size)\n",
    "        self.smallRNN = RNN(embedding_size, dprime, output_size)\n",
    "\n",
    "        self.d = d\n",
    "        self.dprime = dprime\n",
    "        \n",
    "        self.skimcount = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        input = torch.cat( (embedded, hidden), 2).view(1,-1)\n",
    "        p = self.selector(input)\n",
    "        q = p.exp().multinomial()\n",
    "        if q.data[0,0]==0:\n",
    "            #main RNN\n",
    "            output, hidden = self.mainRNN(embedded, hidden)\n",
    "        else:\n",
    "            #small RNN\n",
    "            \n",
    "            # ce n'est plus l'état caché entier qui passe dans le petit RNN, mais le slice sur les dprime premiers\n",
    "            # élements. Pas sûr de mon coup mais je vois pas comment ça peut marcher avec les dimensions si on a\n",
    "            # un état caché de taille d en entrée dans le petit RNN. \n",
    "            # hidden décomposé en h0 (dprime premiers éléments) et h1\n",
    "            \n",
    "            h0 = hidden.view(-1)[:self.dprime]\n",
    "            output, h0 = self.smallRNN(embedded, h0.view(1,1,-1))\n",
    "            h1 = hidden.view(-1)[self.dprime:]\n",
    "            hidden = torch.cat( (h0.view(1,1,-1), h1.view(1,1,-1)), 2)\n",
    "            self.skimcount += 1\n",
    "        self.count += 1\n",
    "        return output, p, hidden\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.rand(1, 1, self.d))\n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonctions pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel():\n",
    "    return -torch.log(-torch.log(torch.FloatTensor(2).uniform_()))\n",
    "\n",
    "def r(logp, g, temperature):\n",
    "    num = [torch.exp( (pi + gi)/temperature) for (pi,gi) in zip(logp,g)]\n",
    "    denum = torch.sum(torch.exp( (logp.data + g)/temperature))\n",
    "    return [n.data/denum for n in num]\n",
    "\n",
    "\n",
    "def train(preprocesser, input,target,rnn, optimizers, criterion, temperature=0.5, gamma=1e-2):\n",
    "    '''todo : commenter les inputs'''\n",
    "    for o in optimizers:\n",
    "        o.zero_grad()\n",
    "        \n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    input_length = input.size()[0]\n",
    "    logp = []\n",
    "    rp = []\n",
    "    loss = 0\n",
    "    loss_output, loss_linear = 0, 0\n",
    "    \n",
    "    for t in range(input_length):\n",
    "        x = input[t]\n",
    "        output, p, hidden = rnn(Variable(x), hidden)\n",
    "        logp.append(p.data)\n",
    "        g = gumbel()    \n",
    "        rp.append(r(p,g,temperature))\n",
    "    \n",
    "        c = criterion(output.view(1,-1), target)\n",
    "        loss = criterion(output.view(1,-1), target).data \n",
    "        loss_output += loss\n",
    "        loss_linear += loss * rp[t][0].prod()\n",
    "    logp_skim = [l[0,1] for l in logp]   #get logp probabibilites of skimming for each word\n",
    "    Variable(loss_output, requires_grad=True).backward()\n",
    "    loss_linear = Variable( loss_linear +(gamma/input_length) * np.sum(logp_skim), requires_grad=True)\n",
    "    loss_linear.backward()\n",
    "\n",
    "#     for p in rnn.parameters():\n",
    "#         print p\n",
    "#     for o in optimizers:\n",
    "#         print o\n",
    "#         o.step()\n",
    "#     for p in rnn.parameters():\n",
    "#         print p\n",
    "    print ber\n",
    "    return output ,loss_output[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamètres + apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.optim.adam.Adam object at 0x7f7fed6699d0>\n",
      "<torch.optim.sgd.SGD object at 0x7f7fed669b10>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'ber' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-cc59fd86ec08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mpreprocesser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmakeInputTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocesser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocesser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_linear\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-49da242d0458>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(preprocesser, input, target, rnn, optimizers, criterion, temperature, gamma)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#     for p in rnn.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#         print p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mloss_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'ber' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "rnn =  SkimRNN(preprocesser.n_words, 512, 200, 5, 2, 2)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "optimizer_rnn = optim.Adam([{'params':rnn.smallRNN.parameters()},{'params':rnn.mainRNN.parameters()}], lr=1e-2)\n",
    "optimizer_linear = optim.SGD(rnn.selector.parameters(), lr=1e-2)\n",
    "\n",
    "epoch,mod = 0,5\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i in train_indexes[:500]:\n",
    "        s,t =preprocesser.corpus[i], sentiment[i]\n",
    "        input,target =  makeInputTarget(preprocesser, s, t)\n",
    "        output, l = train(preprocesser,input,target,rnn,[optimizer_rnn, optimizer_linear],criterion,0.5)\n",
    "\n",
    "        \n",
    "\n",
    "    if epoch%mod == 0:\n",
    "        losses.append(l)\n",
    "        accs.append(accuracy(test_indexes[:200],preprocesser,rnn))\n",
    "        print epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- step qui màj les paramètres (backward graph?)\n",
    "          \n",
    "- inputs via glove\n",
    "\n",
    "\n",
    "- vérifier résultats après apprentissage\n",
    "\n",
    "\n",
    "- faire des batchs\n",
    "\n",
    "\n",
    "- implémenter skim rnn avec option lstm\n",
    "\n",
    "\n",
    "- implémenter fonctions d'évaluations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(indexes,preprocesser,rnn):\n",
    "    acc = 0.0\n",
    "    for i in indexes:\n",
    "        s,t =preprocesser.corpus[i], sentiment[i]\n",
    "        input,target =  makeInputTarget(preprocesser, s, t)\n",
    "        input_length = input.size()[0]\n",
    "        \n",
    "        for word in range(input_length):\n",
    "            hidden = rnn.initHidden()\n",
    "            x = input[word]\n",
    "            output, p, hidden = rnn(Variable(x), hidden)  \n",
    "        if output.exp().multinomial().data[0,0] == target.data[0]:\n",
    "            acc += 1\n",
    "            \n",
    "    return 100*float(acc/len(indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47.5,\n",
       " 51.0,\n",
       " 55.50000000000001,\n",
       " 56.99999999999999,\n",
       " 55.50000000000001,\n",
       " 54.50000000000001,\n",
       " 52.0,\n",
       " 48.0,\n",
       " 58.5,\n",
       " 50.5,\n",
       " 55.00000000000001,\n",
       " 52.5,\n",
       " 48.5,\n",
       " 48.0,\n",
       " 47.0,\n",
       " 56.00000000000001,\n",
       " 45.0,\n",
       " 53.0,\n",
       " 55.50000000000001,\n",
       " 59.0]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166776921986486"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0*rnn.skimcount/rnn.count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
